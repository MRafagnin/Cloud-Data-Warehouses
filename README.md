# Database and ETL Pipeline for Sparkify

## Objective
> Build ETL pipeline that extracts data from S3, stages in Redshift, and transforms into a set of dimensional tables for Sparkify Analytics Team

## Table of contents
* [Technologies](#technologies)
* [Launch](#launch)
* [Database Schema](#database-schema)
* [Datasets](#datasets)
* [Files in Repository](#files-in-repository)
* [IMPORTANT NOTES](#important-notes)

## Technologies
Project is created with:
- Jupyter Notebook
- Python 3
- AWS
- AWS Redshift

## Launch
- Run code line in `start.ipynb`

## Database Schema

#### Fact Table:
        
##### songplays - [records in log data associated with song plays]
- songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
                    
#### Dimension Tables:

##### users - [users in the app]
- user_id, first_name, last_name, gender, level
            
##### songs - [songs in music database]
- song_id, title, artist_id, year, duration
            
##### artists - [artists in music database]
- artist_id, name, location, latitude, longitude (ie. [Artists_table](/readme_images/artists_table.png))

##### time - [timestamps of records in songplays broken down into specific units]
- start_time, hour, day, week, month, year, weekday (ie. [Time_table](/readme_images/time_table.png))

#### Staging Tables:
##### staging_events - [data from S3 in loaded into this table on Redshift]
- artist, auth, firstName, gender, ItemInSession, lastName, lenght, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId

##### staging_songs - - [data from S3 in loaded into this table on Redshift]
- start_time, user_id, level, song_id , artist_id, session_id, location, user_agent


## Datasets

### Song Dataset
- Subset of data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/)
- JSON files in S3 contain metadata about songs and artists

### Log Dataset
- JSON log files generated by [eventsim](https://github.com/Interana/eventsim) based on above dataset

## Files/Folder in Repository

- `start.ipynb` - executes `create_tables.py` and `etl.py`

- `create_tables.py` - drops and creates tables. Run this file to reset your tables before each time you run your ETL scripts.

- `etl.py` - load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift.

- `sql_queries.py` - contains all your sql queries, and is imported into the last files above.

- `dwh.cfg` - AWS access pointers (fill accordantly)

- **readme_images** - contains images that link to this README file

## IMPORTANT NOTES:

> Add Redshift database and IAM role info to `dwh.cfg`

> You will not be able to run `etl.py` until you have run `create_tables.py` at least once to create the tables and access database.

> Reminder: Delete your Redshift cluster when finished (if applicable)

> Code references taken from 'Anshul R' via the Knowledge platform